{\rtf1\ansi\ansicpg1252\cocoartf2509
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 // MP Reduction\
// Given a list (lst) of length n\
// Output its sum = lst[0] + lst[1] + ... + lst[n-1];\
\
#include <wb.h>\
\
#define BLOCK_SIZE 512 //@@ You can change this\
\
#define wbCheck(stmt)                                                     \\\
  do \{                                                                    \\\
    cudaError_t err = stmt;                                               \\\
    if (err != cudaSuccess) \{                                             \\\
      wbLog(ERROR, "Failed to run stmt ", #stmt);                         \\\
      wbLog(ERROR, "Got CUDA error ...  ", cudaGetErrorString(err));      \\\
      return -1;                                                          \\\
    \}                                                                     \\\
  \} while (0)\
\
__global__ void totalGPU(float *input, float *output, int len) \{\
  //@@ Load a segment of the input vector into shared memory\
  __shared__ float partialSum[2 * BLOCK_SIZE];\
  unsigned int tid = threadIdx.x;\
  unsigned int start = 2 * blockIdx.x * blockDim.x;\
  partialSum[tid] = (start + tid) < len ? input[start + tid]:0.;\
  partialSum[blockDim.x + tid] = (start + blockDim.x + tid) < len? input[start + blockDim.x + tid]:0.;\
  //@@ Traverse the reduction tree\
  for (unsigned int stride = blockDim.x; stride >= 1; stride >>= 1)\{\
    __syncthreads();\
    if (tid < stride)\{\
      partialSum[tid] += partialSum[tid + stride];\
    \}\
  \}\
  __syncthreads();\
  //@@ Write the computed sum of the block to the output vector at the\
  //@@ correct index\
  if (tid == 0)\{\
    output[blockIdx.x] = partialSum[0];\
  \}\
\}\
\
int main(int argc, char **argv) \{\
  int ii;\
  wbArg_t args;\
  float *hostInput;  // The input 1D list\
  float *hostOutput; // The output list\
  float *deviceInput;\
  float *deviceOutput;\
  int numInputElements;  // number of elements in the input list\
  int numOutputElements; // number of elements in the output list\
\
  args = wbArg_read(argc, argv);\
\
  wbTime_start(Generic, "Importing data and creating memory on host");\
  hostInput =\
      (float *)wbImport(wbArg_getInputFile(args, 0), &numInputElements);\
\
  numOutputElements = numInputElements / (BLOCK_SIZE << 1);\
  if (numInputElements % (BLOCK_SIZE << 1)) \{\
    numOutputElements++;\
  \}\
  hostOutput = (float *)malloc(numOutputElements * sizeof(float));\
\
  wbTime_stop(Generic, "Importing data and creating memory on host");\
\
  wbLog(TRACE, "The number of input elements in the input is ",\
        numInputElements);\
  wbLog(TRACE, "The number of output elements in the input is ",\
        numOutputElements);\
\
  wbTime_start(GPU, "Allocating GPU memory.");\
  //@@ Allocate GPU memory here\
  cudaMalloc((void **)&deviceInput, numInputElements * sizeof(float));\
  cudaMalloc((void **)&deviceOutput, numOutputElements * sizeof(float));\
  \
  wbTime_stop(GPU, "Allocating GPU memory.");\
\
  wbTime_start(GPU, "Copying input memory to the GPU.");\
  //@@ Copy memory to the GPU here\
  cudaMemcpy(deviceInput, hostInput, numInputElements * sizeof(float), cudaMemcpyHostToDevice);\
  cudaMemset(deviceOutput, 0., numOutputElements * sizeof(float));\
  \
  wbTime_stop(GPU, "Copying input memory to the GPU.");\
  //@@ Initialize the grid and block dimensions here\
  dim3 dimBlock(BLOCK_SIZE);\
  dim3 dimGrid(numOutputElements);\
  \
  wbTime_start(Compute, "Performing CUDA computation");\
  //@@ Launch the GPU Kernel here\
  totalGPU <<<dimGrid, dimBlock>>> (deviceInput, deviceOutput, numInputElements);\
  cudaDeviceSynchronize();\
  wbTime_stop(Compute, "Performing CUDA computation");\
\
  wbTime_start(Copy, "Copying output memory to the CPU");\
  //@@ Copy the GPU memory back to the CPU here\
  cudaMemcpy(hostOutput, deviceOutput, numOutputElements * sizeof(float), cudaMemcpyDeviceToHost);\
  wbTime_stop(Copy, "Copying output memory to the CPU");\
\
  /********************************************************************\
   * Reduce output vector on the host\
   * NOTE: One could also perform the reduction of the output vector\
   * recursively and support any size input. For simplicity, we do not\
   * require that for this lab.\
   ********************************************************************/\
  for (ii = 1; ii < numOutputElements; ii++) \{\
    hostOutput[0] += hostOutput[ii];\
  \}\
\
  wbTime_start(GPU, "Freeing GPU Memory");\
  //@@ Free the GPU memory here\
  cudaFree(deviceInput);\
  cudaFree(deviceOutput);\
  wbTime_stop(GPU, "Freeing GPU Memory");\
\
  wbSolution(args, hostOutput, 1);\
\
  free(hostInput);\
  free(hostOutput);\
\
  return 0;\
\}}